{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up default Graph() of tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=1):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Managing Computation Graphs in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining computation graph\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x * x * y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct Way to Run a Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph will not run until executed by session\n",
    "sess = tf.Session()\n",
    "# variables need to be initiated\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "# once all variables are initiated, run f\n",
    "result = sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use context manager with Session to run a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively, define a \"variable initializer\" node in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare an init node\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # init node initializes all Variables\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing graphs when more than 1 is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of the 3 nodes' `.graph` attribute point to the exact same computation graph (default graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# graph of f created above is the default graph\n",
    "print(f.graph is tf.get_default_graph())\n",
    "print(x.graph is tf.get_default_graph())\n",
    "print(y.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly creating a new Graph() to store nodes\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    # below line will raise exception since y is a node in\n",
    "    # another graph\n",
    "    #f2 = x2 * 3 * y\n",
    "    f2 = x2 * x2 * 3\n",
    "    # init will only initialize Variables **before** it\n",
    "    # before to have init latest of the pack\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph needs to be defined explicitly here\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init.run()\n",
    "    result = f2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the default graph was only teporarily set as graph\n",
    "graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lifecycle of Node Values\n",
    "\n",
    "Node values are only computed when running a evaluation through the graph, and are discarded once the computation is done. Only `Variable` values are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    w = tf.constant(10)\n",
    "    x = w + 2\n",
    "    y = x + 4\n",
    "    z = x * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph1) as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Running Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick pull a sample dataset before running a TF Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "# obtain m and n, m is number of records and n is number of columns\n",
    "m, n = housing.data.shape\n",
    "# add 1s to the matrix for each row, which is for model bias / intercept\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing_data_plus_bias)\n",
    "scaled_housing_data_plus_bias = scaler.transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "8\n",
      "[[ 0.          2.34476576  0.98214266  0.62855945 -0.15375759 -0.9744286\n",
      "  -0.04959654  1.05254828 -1.32783522]\n",
      " [ 0.          2.33223796 -0.60701891  0.32704136 -0.26333577  0.86143887\n",
      "  -0.09251223  1.04318455 -1.32284391]]\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "print(n)\n",
    "print(scaled_housing_data_plus_bias[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Computation Graph using Normal Equation for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# load Variable as constant tensors\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "# Calculate theta using the normal equation\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF Computation Graph using Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Manual Computation of Gradient Descent (Batch GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "**Rescaling input data by column (feature) such that features all have same scale. This is VITAL for any GD solution!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the construction phase - build the `Graph()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define hyper parameter\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# load training data\n",
    "# note that X and y are computed directly, indicating this is a\n",
    "# Batch GD\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "# start a random theta for GB. It should have (n + 1) elements\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=1), name='theta')\n",
    "\n",
    "# calculate y_pred\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "# calculate error\n",
    "error = y_pred - y\n",
    "# calculate mean square error\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "\n",
    "# calculate gradients of the mse (loss function)\n",
    "gradients = 2 / m * tf.matmul(tf.transpose(X), error)\n",
    "\n",
    "# define GD update formula\n",
    "# note: tf.assign updates the Variable with new theta\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# last but not least, create a init node\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the execution phase - run the `Graph()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 8.274792671203613\n",
      "Epoch 100: MSE = 4.990600109100342\n",
      "Epoch 200: MSE = 4.881271839141846\n",
      "Epoch 300: MSE = 4.860585689544678\n",
      "Epoch 400: MSE = 4.848357200622559\n",
      "Epoch 500: MSE = 4.839105129241943\n",
      "Epoch 600: MSE = 4.831891059875488\n",
      "Epoch 700: MSE = 4.826225757598877\n",
      "Epoch 800: MSE = 4.821752548217773\n",
      "Epoch 900: MSE = 4.818204402923584\n",
      "[[-0.5219252 ]\n",
      " [ 0.9154371 ]\n",
      " [ 0.15288655]\n",
      " [-0.3939141 ]\n",
      " [ 0.3970172 ]\n",
      " [ 0.00679991]\n",
      " [-0.04404339]\n",
      " [-0.57045436]\n",
      " [-0.54946506]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            # note here when mse is evaluated, it does not call learning_op\n",
    "            # which actually updates the theta\n",
    "            print(\"Epoch {0}: MSE = {1}\".format(epoch, mse.eval()))\n",
    "        # note that training_op is run here which updates theta in ach epoch\n",
    "        sess.run(training_op)\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using TF's `autodiff` features to compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `tf.gradients()` to automatically calculate the gradient based on the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define hyper parameter\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# load training data\n",
    "# note that X and y are computed directly, indicating this is a\n",
    "# Batch GD\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "# start a random theta for GB. It should have (n + 1) elements\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=1), name='theta')\n",
    "\n",
    "# calculate y_pred\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "# calculate error\n",
    "error = y_pred - y\n",
    "# calculate mean square error\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "\n",
    "# replace hand-written gradient computation with\n",
    "# tf.gradients() returns a list of gradients with regard\n",
    "# to each x\n",
    "gradients = tf.gradients(ys=[mse], xs=[theta])[0]\n",
    "\n",
    "# define GD update formula\n",
    "# note: tf.assign updates the Variable with new theta\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# last but not least, create a init node\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 8.274792671203613\n",
      "Epoch 100: MSE = 4.990600109100342\n",
      "Epoch 200: MSE = 4.881271839141846\n",
      "Epoch 300: MSE = 4.860585689544678\n",
      "Epoch 400: MSE = 4.848357200622559\n",
      "Epoch 500: MSE = 4.839105129241943\n",
      "Epoch 600: MSE = 4.831891059875488\n",
      "Epoch 700: MSE = 4.826225757598877\n",
      "Epoch 800: MSE = 4.821752548217773\n",
      "Epoch 900: MSE = 4.818204402923584\n",
      "[[-0.5219252 ]\n",
      " [ 0.9154371 ]\n",
      " [ 0.15288658]\n",
      " [-0.3939141 ]\n",
      " [ 0.3970172 ]\n",
      " [ 0.00679992]\n",
      " [-0.0440434 ]\n",
      " [-0.57045406]\n",
      " [-0.5494649 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            # note here when mse is evaluated, it does not call learning_op\n",
    "            # which actually updates the theta\n",
    "            print(\"Epoch {0}: MSE = {1}\".format(epoch, mse.eval()))\n",
    "        # note that training_op is run here which updates theta in ach epoch\n",
    "        sess.run(training_op)\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Using TF's native optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define hyper parameter\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# load training data\n",
    "# note that X and y are computed directly, indicating this is a\n",
    "# Batch GD\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "# start a random theta for GB. It should have (n + 1) elements\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=1), name='theta')\n",
    "\n",
    "# calculate y_pred\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "# calculate error\n",
    "error = y_pred - y\n",
    "# calculate mean square error\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace hand-written gradient computation with\n",
    "# tf native optimizer that calculates gradients automatically\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last but not least, create a init node\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 8.274792671203613\n",
      "Epoch 100: MSE = 4.815185546875\n",
      "Epoch 200: MSE = 4.80463981628418\n",
      "Epoch 300: MSE = 4.8034348487854\n",
      "Epoch 400: MSE = 4.803277969360352\n",
      "Epoch 500: MSE = 4.803256511688232\n",
      "Epoch 600: MSE = 4.803253650665283\n",
      "Epoch 700: MSE = 4.803253650665283\n",
      "Epoch 800: MSE = 4.8032546043396\n",
      "Epoch 900: MSE = 4.8032546043396\n",
      "[[-0.5219252 ]\n",
      " [ 0.8296325 ]\n",
      " [ 0.11875413]\n",
      " [-0.26555166]\n",
      " [ 0.30571672]\n",
      " [-0.00450233]\n",
      " [-0.03932676]\n",
      " [-0.8998556 ]\n",
      " [-0.87051237]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            # note here when mse is evaluated, it does not call learning_op\n",
    "            # which actually updates the theta\n",
    "            print(\"Epoch {0}: MSE = {1}\".format(epoch, mse.eval()))\n",
    "        # note that training_op is run here which updates theta in ach epoch\n",
    "        sess.run(training_op)\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mini-batch GD Using `tf.placeholder` ops to train Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define hyper parameter\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mini batch size and number of batches\n",
    "mini_batch_size = 1000\n",
    "n_batches = int(np.ceil(m / mini_batch_size))\n",
    "\n",
    "# load training data\n",
    "# note that X and y are shaped placeholder ready to take\n",
    "# mini batch at execution phase\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a random theta for GB. It should have (n + 1) elements\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=1), name='theta')\n",
    "\n",
    "# calculate y_pred\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "# calculate error\n",
    "error = y_pred - y\n",
    "# calculate mean square error\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "\n",
    "# replace hand-written gradient computation with\n",
    "# tf native optimizer that calculates gradients automatically\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# last but not least, create a init node\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(n_records, batch_index, batch_size):\n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = min((batch_index + 1) * batch_size, n_records)\n",
    "    X_batch = scaled_housing_data_plus_bias[start_index:end_index, :]\n",
    "    y_batch = housing.target.reshape(-1, 1)[start_index:end_index, :]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "`.eval()` method for any computation node will need a `feed_dict` if the note is depend on any `placeholder` ops along its graph path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE: 5.323634147644043\n",
      "Epoch 100 MSE: 4.386706352233887\n",
      "Epoch 200 MSE: 4.339206218719482\n",
      "Epoch 300 MSE: 4.332136154174805\n",
      "Epoch 400 MSE: 4.331105709075928\n",
      "Epoch 500 MSE: 4.33095645904541\n",
      "Epoch 600 MSE: 4.330934047698975\n",
      "Epoch 700 MSE: 4.330931186676025\n",
      "Epoch 800 MSE: 4.330931186676025\n",
      "Epoch 900 MSE: 4.330931186676025\n",
      "[[-0.5219252 ]\n",
      " [ 0.86755925]\n",
      " [ 0.10342795]\n",
      " [-0.28372166]\n",
      " [ 0.33215836]\n",
      " [ 0.00840846]\n",
      " [-0.03587097]\n",
      " [-0.8639838 ]\n",
      " [-0.9650578 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch {0} MSE: {1}\".format(epoch, mse.eval(feed_dict={X:X_batch, y:y_batch})))\n",
    "        for batch_index in range(n_batches):\n",
    "            # obtain batch data from the source\n",
    "            X_batch, y_batch = fetch_data(m, batch_index, batch_size=mini_batch_size)\n",
    "            # feed the batch data to the placeholder ops\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF's built in serialization tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define hyper parameter\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "mini_batch_size = 1000\n",
    "n_batches = int(np.ceil(m / mini_batch_size))\n",
    "\n",
    "# load training data\n",
    "# note that X and y are shaped placeholder ready to take\n",
    "# mini batch at execution phase\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "# start a random theta for GB. It should have (n + 1) elements\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=1), name='theta')\n",
    "\n",
    "# calculate y_pred\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "\n",
    "# calculate error\n",
    "error = y_pred - y\n",
    "# calculate mean square error\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "\n",
    "# replace hand-written gradient computation with\n",
    "# tf native optimizer that calculates gradients automatically\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# last but not least, create a init node\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a Saver() node at the end of the graph construction phase\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE: 5.323634147644043\n",
      "Epoch 100 MSE: 4.386706352233887\n",
      "Epoch 200 MSE: 4.339206218719482\n",
      "Epoch 300 MSE: 4.332136154174805\n",
      "Epoch 400 MSE: 4.331105709075928\n",
      "Epoch 500 MSE: 4.33095645904541\n",
      "Epoch 600 MSE: 4.330934047698975\n",
      "Epoch 700 MSE: 4.330931186676025\n",
      "Epoch 800 MSE: 4.330931186676025\n",
      "Epoch 900 MSE: 4.330931186676025\n",
      "[[-0.5219252 ]\n",
      " [ 0.86755925]\n",
      " [ 0.10342795]\n",
      " [-0.28372166]\n",
      " [ 0.33215836]\n",
      " [ 0.00840846]\n",
      " [-0.03587097]\n",
      " [-0.8639838 ]\n",
      " [-0.9650578 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch {0} MSE: {1}\".format(epoch, mse.eval(feed_dict={X:X_batch, y:y_batch})))\n",
    "            # save the model to a path\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        for batch_index in range(n_batches):\n",
    "            # obtain batch data from the source\n",
    "            X_batch, y_batch = fetch_data(m, batch_index, batch_size=mini_batch_size)\n",
    "            # feed the batch data to the placeholder ops\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "    save_path = saver.save(sess, \"/tmp/my_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The `Saver` node saves the needed variables in the path (in this case, it is the `theta` Variable). When serialization is needed, call `Saver().restore()` to use the stored model values instead of the `init` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 22:40:43.425536 16560 deprecation.py:323] From C:\\Users\\oycy\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE: 4.330931186676025\n",
      "Epoch 100 MSE: 4.330931186676025\n",
      "Epoch 200 MSE: 4.330931186676025\n",
      "Epoch 300 MSE: 4.330931186676025\n",
      "Epoch 400 MSE: 4.330931186676025\n",
      "Epoch 500 MSE: 4.330931186676025\n",
      "Epoch 600 MSE: 4.330931186676025\n",
      "Epoch 700 MSE: 4.330931186676025\n",
      "Epoch 800 MSE: 4.330931186676025\n",
      "Epoch 900 MSE: 4.330931186676025\n",
      "[[-0.5219252 ]\n",
      " [ 0.86755925]\n",
      " [ 0.10342795]\n",
      " [-0.28372166]\n",
      " [ 0.33215836]\n",
      " [ 0.00840846]\n",
      " [-0.03587097]\n",
      " [-0.8639838 ]\n",
      " [-0.9650578 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #sess.run(init)\n",
    "    saver.restore(sess, '/tmp/my_model.ckpt')\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch {0} MSE: {1}\".format(epoch, mse.eval(feed_dict={X:X_batch, y:y_batch})))\n",
    "            # save the model to a path\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        for batch_index in range(n_batches):\n",
    "            # obtain batch data from the source\n",
    "            X_batch, y_batch = fetch_data(m, batch_index, batch_size=mini_batch_size)\n",
    "            # feed the batch data to the placeholder ops\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "    save_path = saver.save(sess, \"/tmp/my_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, **the whole graph structure** can be loaded instead, and the Variable states can be restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the graph structure to default graph\n",
    "saver = tf.train.import_meta_graph('/tmp/my_model.ckpt.meta')\n",
    "# obtains the Variable\n",
    "theta = tf.get_default_graph().get_tensor_by_name('theta:0')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5219252 ]\n",
      " [ 0.86755925]\n",
      " [ 0.10342795]\n",
      " [-0.28372166]\n",
      " [ 0.33215836]\n",
      " [ 0.00840846]\n",
      " [-0.03587097]\n",
      " [-0.8639838 ]\n",
      " [-0.9650578 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    #sess.run(init)\n",
    "    saver.restore(sess, '/tmp/my_model.ckpt')\n",
    "    best_theta_restored = theta.eval()\n",
    "    print(best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a dynamic directory creation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = 'tf_logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the computation graph as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "mini_batch_size = 100\n",
    "n_batches = int(np.ceil(m / mini_batch_size))\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=1), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define logging system. The `summary` is a TensorBoard compatible format that logs information for Tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the summary scalar records the mse to a summary object\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "# the FileWriter writes the information to a designated location,\n",
    "# for the currently default graph\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add actions in the execution phase to record the mse along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE: 3.4942097663879395\n",
      "Epoch 100 MSE: nan\n",
      "Epoch 200 MSE: nan\n",
      "Epoch 300 MSE: nan\n",
      "Epoch 400 MSE: nan\n",
      "Epoch 500 MSE: nan\n",
      "Epoch 600 MSE: nan\n",
      "Epoch 700 MSE: nan\n",
      "Epoch 800 MSE: nan\n",
      "Epoch 900 MSE: nan\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch {0} MSE: {1}\".format(epoch, mse.eval(feed_dict={X:X_batch, y:y_batch})))\n",
    "        for batch_index in range(n_batches):\n",
    "            # obtain batch data from the source\n",
    "            X_batch, y_batch = fetch_data(m, batch_index, batch_size=mini_batch_size)\n",
    "            # create a logging system that writes performance while training\n",
    "            if batch_index % 10 == 0:\n",
    "                # the summary str is calculated. Notice that mse_summary\n",
    "                # knows to look for mse to eval\n",
    "                summary_str = mse_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                # step is total number of minibatches from beginning\n",
    "                step = epoch * n_batches + batch_index\n",
    "                # call the file_writer to add the above information\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            # feed the batch data to the placeholder ops\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "    # after all epochs are done, the theta has been assigned n_epoch times\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection of the writer to the disk\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The records of MSE scalar is now traceable in Tensorboard**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf35",
   "language": "python",
   "name": "tf35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
