{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=1):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf implementation of Batch Normalization (BN) layers (mostly for vanishing gradient issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Major net parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. construct computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "In graph construct phase, for Batch Normalization graph, the main change is:\n",
    "\n",
    "The need of a `train` placeholder node with default value as \"False\" to control if the `tf.layers.batch_normalization()` layers should use the running stats of the mini batches or the whole data stats in calculation. The former is for model training and the latter is for model evaluation / apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder node in the computation graph\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y will be just 1D tensor, int\n",
    "\n",
    "# a controller node in the graph, giving a signal to BN nodes, such that\n",
    "# it uses different parameters (mean and std dev) of mini-batch or whole data\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope('fc'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    # this function takes labels from y, one hot it, and then use the logits node to \n",
    "    # calculate loss\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # average xentropy as the \"loss\" of the model current state\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    # tells if the logits perdictions are in top 1\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# define logging systems\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = 'mlp_logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "with tf.name_scope('log'):\n",
    "    acc_summary = tf.summary.scalar('Acc', accuracy)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use `partial()` to define the BN with the same settings and use in the graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0822 13:26:14.260378 15428 deprecation.py:323] From <ipython-input-8-415c75686bfa>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0822 13:26:14.267363 15428 deprecation.py:506] From C:\\Users\\oycy\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0822 13:26:14.660347 15428 deprecation.py:323] From <ipython-input-8-415c75686bfa>:16: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    }
   ],
   "source": [
    "# tip: using python native functools.partial() to wrap a function with some default values\n",
    "from functools import partial\n",
    "\n",
    "# define placeholder node in the computation graph\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y will be just 1D tensor, int\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# define computation graph\n",
    "\n",
    "# create a bn function with same parameters that's repeated\n",
    "my_bn_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "with tf.name_scope('fc'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
    "    bn1 = my_bn_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
    "    bn2 = my_bn_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
    "    logits = my_bn_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    # this function takes labels from y, one hot it, and then use the logits node to \n",
    "    # calculate loss\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # average xentropy as the \"loss\" of the model current state\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    # tells if the logits perdictions are in top 1\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# define logging systems\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = 'mlp_logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "with tf.name_scope('log'):\n",
    "    acc_summary = tf.summary.scalar('Acc', accuracy)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 13:26:24.274164 15428 deprecation.py:323] From <ipython-input-9-4141630e56b4>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0822 13:26:24.289788 15428 deprecation.py:323] From C:\\Users\\oycy\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0822 13:26:25.937195 15428 deprecation.py:323] From C:\\Users\\oycy\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0822 13:26:26.530279 15428 deprecation.py:323] From C:\\Users\\oycy\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0822 13:26:26.691383 15428 deprecation.py:323] From C:\\Users\\oycy\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "In the execution phase, 2 things to note for running a graph with BN layers:\n",
    "\n",
    "1. The training runs need to feed additional `True` value to the `training` node to overwrite the default `False`. This will help trigger all the BN layers to calculate stats based on mini-batches\n",
    "2. In training process, the BN layers need to dynamically update the parameters for input data; however these parameters are **not** updated automatically. A `tf.GraphKeys.UPDATE_OPS` collection of ops should be executed with the exact same parameters with the `training_op` in training, and it should run after the `training_op` in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Accuracy - 0.925000011920929\n",
      "Epoch 2: Val Accuracy - 0.9462000131607056\n",
      "Epoch 3: Val Accuracy - 0.9557999968528748\n",
      "Epoch 4: Val Accuracy - 0.9631999731063843\n",
      "Epoch 5: Val Accuracy - 0.9635999798774719\n",
      "Epoch 6: Val Accuracy - 0.9679999947547913\n",
      "Epoch 7: Val Accuracy - 0.9696000218391418\n",
      "Epoch 8: Val Accuracy - 0.9714000225067139\n",
      "Epoch 9: Val Accuracy - 0.9732000231742859\n",
      "Epoch 10: Val Accuracy - 0.974399983882904\n",
      "Epoch 11: Val Accuracy - 0.9732000231742859\n",
      "Epoch 12: Val Accuracy - 0.9757999777793884\n",
      "Epoch 13: Val Accuracy - 0.977400004863739\n",
      "Epoch 14: Val Accuracy - 0.9746000170707703\n",
      "Epoch 15: Val Accuracy - 0.9768000245094299\n",
      "Epoch 16: Val Accuracy - 0.9769999980926514\n",
      "Epoch 17: Val Accuracy - 0.977400004863739\n",
      "Epoch 18: Val Accuracy - 0.9783999919891357\n",
      "Epoch 19: Val Accuracy - 0.9775999784469604\n",
      "Epoch 20: Val Accuracy - 0.9775999784469604\n",
      "Epoch 21: Val Accuracy - 0.977400004863739\n",
      "Epoch 22: Val Accuracy - 0.9782000184059143\n",
      "Epoch 23: Val Accuracy - 0.9793999791145325\n",
      "Epoch 24: Val Accuracy - 0.979200005531311\n",
      "Epoch 25: Val Accuracy - 0.9818000197410583\n",
      "Epoch 26: Val Accuracy - 0.9801999926567078\n",
      "Epoch 27: Val Accuracy - 0.9796000123023987\n",
      "Epoch 28: Val Accuracy - 0.9789999723434448\n",
      "Epoch 29: Val Accuracy - 0.9805999994277954\n",
      "Epoch 30: Val Accuracy - 0.9796000123023987\n",
      "Epoch 31: Val Accuracy - 0.9800000190734863\n",
      "Epoch 32: Val Accuracy - 0.9811999797821045\n",
      "Epoch 33: Val Accuracy - 0.979200005531311\n",
      "Epoch 34: Val Accuracy - 0.9807999730110168\n",
      "Epoch 35: Val Accuracy - 0.9814000129699707\n",
      "Epoch 36: Val Accuracy - 0.9818000197410583\n",
      "Epoch 37: Val Accuracy - 0.9807999730110168\n",
      "Epoch 38: Val Accuracy - 0.9810000061988831\n",
      "Epoch 39: Val Accuracy - 0.9807999730110168\n",
      "Epoch 40: Val Accuracy - 0.9819999933242798\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            if iteration % 10 == 0:\n",
    "                summary_str = acc_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                # step is total number of minibatches from beginning\n",
    "                step = epoch * (mnist.train.num_examples // batch_size + 1) + iteration\n",
    "                # call the file_writer to add the above information\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training:True, X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"Epoch {0}: Val Accuracy - {1}\".format(\n",
    "            epoch + 1, acc_val))\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Gradient Clipping for Exploding Gradients issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Use the exact same graph from above design, however, gradient clipping needs to happen on the tf optimizer such that:\n",
    "\n",
    "1. The `.minimize()` method automatically calculates the gradients and then update the weights\n",
    "2. Instead:\n",
    "    1. Use `optimizer.compute_gradients()` to obtain the gradients\n",
    "    2. Then use tf function `clip_by_value()` to clip the gradients\n",
    "    3. Take the clipped gradients and use `optimizer.apply_gradients()` to update the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "# for gradient clipping\n",
    "gradient_threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip: using python native functools.partial() to wrap a function with some default values\n",
    "from functools import partial\n",
    "\n",
    "# define placeholder node in the computation graph\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y will be just 1D tensor, int\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# define computation graph\n",
    "\n",
    "# create a bn function with same parameters that's repeated\n",
    "my_bn_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "with tf.name_scope('fc'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
    "    bn1 = my_bn_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name='hidden2')\n",
    "    bn2 = my_bn_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name='outputs')\n",
    "    logits = my_bn_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    # this function takes labels from y, one hot it, and then use the logits node to \n",
    "    # calculate loss\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # average xentropy as the \"loss\" of the model current state\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "# notice in training here, the grads and vars come in as a list of\n",
    "# (grad, var) tuples. Therefore, the clipped grads should come in\n",
    "# with the exact same data structure\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_vars = [(tf.clip_by_value(grad, -gradient_threshold, gradient_threshold), var)\n",
    "                   for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_vars)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    # tells if the logits perdictions are in top 1\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# define logging systems\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = 'mlp_logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "with tf.name_scope('log'):\n",
    "    acc_summary = tf.summary.scalar('Acc', accuracy)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to change for the execution stage codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Accuracy - 0.9241999983787537\n",
      "Epoch 2: Val Accuracy - 0.9444000124931335\n",
      "Epoch 3: Val Accuracy - 0.9581999778747559\n",
      "Epoch 4: Val Accuracy - 0.9613999724388123\n",
      "Epoch 5: Val Accuracy - 0.9678000211715698\n",
      "Epoch 6: Val Accuracy - 0.9688000082969666\n",
      "Epoch 7: Val Accuracy - 0.9706000089645386\n",
      "Epoch 8: Val Accuracy - 0.972000002861023\n",
      "Epoch 9: Val Accuracy - 0.9742000102996826\n",
      "Epoch 10: Val Accuracy - 0.9747999906539917\n",
      "Epoch 11: Val Accuracy - 0.9750000238418579\n",
      "Epoch 12: Val Accuracy - 0.975600004196167\n",
      "Epoch 13: Val Accuracy - 0.9761999845504761\n",
      "Epoch 14: Val Accuracy - 0.9765999913215637\n",
      "Epoch 15: Val Accuracy - 0.9753999710083008\n",
      "Epoch 16: Val Accuracy - 0.9782000184059143\n",
      "Epoch 17: Val Accuracy - 0.9783999919891357\n",
      "Epoch 18: Val Accuracy - 0.9779999852180481\n",
      "Epoch 19: Val Accuracy - 0.9805999994277954\n",
      "Epoch 20: Val Accuracy - 0.978600025177002\n",
      "Epoch 21: Val Accuracy - 0.9782000184059143\n",
      "Epoch 22: Val Accuracy - 0.9782000184059143\n",
      "Epoch 23: Val Accuracy - 0.9789999723434448\n",
      "Epoch 24: Val Accuracy - 0.9796000123023987\n",
      "Epoch 25: Val Accuracy - 0.9819999933242798\n",
      "Epoch 26: Val Accuracy - 0.9797999858856201\n",
      "Epoch 27: Val Accuracy - 0.979200005531311\n",
      "Epoch 28: Val Accuracy - 0.980400025844574\n",
      "Epoch 29: Val Accuracy - 0.9801999926567078\n",
      "Epoch 30: Val Accuracy - 0.9807999730110168\n",
      "Epoch 31: Val Accuracy - 0.9815999865531921\n",
      "Epoch 32: Val Accuracy - 0.9807999730110168\n",
      "Epoch 33: Val Accuracy - 0.9810000061988831\n",
      "Epoch 34: Val Accuracy - 0.9832000136375427\n",
      "Epoch 35: Val Accuracy - 0.9819999933242798\n",
      "Epoch 36: Val Accuracy - 0.9801999926567078\n",
      "Epoch 37: Val Accuracy - 0.9824000000953674\n",
      "Epoch 38: Val Accuracy - 0.9819999933242798\n",
      "Epoch 39: Val Accuracy - 0.9818000197410583\n",
      "Epoch 40: Val Accuracy - 0.9814000129699707\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            if iteration % 10 == 0:\n",
    "                summary_str = acc_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                # step is total number of minibatches from beginning\n",
    "                step = epoch * (mnist.train.num_examples // batch_size + 1) + iteration\n",
    "                # call the file_writer to add the above information\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training:True, X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"Epoch {0}: Val Accuracy - {1}\".format(\n",
    "            epoch + 1, acc_val))\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf35",
   "language": "python",
   "name": "tf35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
