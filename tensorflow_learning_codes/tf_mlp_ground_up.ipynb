{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=1):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Major net parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define input data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder node in the computation graph\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y will be just 1D tensor, int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (optional) Define a function that generats a fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a non-input neural layer function\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"\n",
    "    ---INPUT---\n",
    "    X: input computation node of size (n_instances, n_inputs). n_input should be the \n",
    "       n_neurons of previous layer\n",
    "    n_neurons: int, count of neuron in current layer. This number will also be the\n",
    "       n_input of next layer\n",
    "    name: name of the layer to be saved in name_scope\n",
    "    activation: activation function\n",
    "    ---OUTPUT---\n",
    "    Z: resulted computation node output of shape (n_instances, n_neurons)\n",
    "    \"\"\"\n",
    "    # define a name scope such that these computation nodes come under the same name\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        # widely accepted little tweak that just works (for now)\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        # truncated random weights ensures that no large weights are created\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name='kernel')\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function is already provided by `tf.layers.dense`. In production, there is no need to re-invent the wheel for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Construct hidden layers using FC (fully connected) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct graph using customized function\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    # the \"logits\" is pre-softmax node\n",
    "    logits = neuron_layer(hidden2, n_output, name='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively using tf.layers.dense\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n",
    "    # the \"logits\" is pre-softmax node\n",
    "    logits = neuron_layer(hidden2, n_output, name='outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define loss function for GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    # this function takes labels from y, one hot it, and then use the logits node to \n",
    "    # calculate loss\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # average xentropy as the \"loss\" of the model current state\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the optimizer that does gradient back propagation based on loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Define the evaluation nodes that calculates based on pure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    # tells if the logits perdictions are in top 1\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Define logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = 'mlp_logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('log'):\n",
    "    acc_summary = tf.summary.scalar('Acc', accuracy)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Define init and saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9064000248908997\n",
      "Epoch 2: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9287999868392944\n",
      "Epoch 3: Train Accuracy - 0.8799999952316284, Val Accuracy - 0.9369999766349792\n",
      "Epoch 4: Train Accuracy - 0.9399999976158142, Val Accuracy - 0.9434000253677368\n",
      "Epoch 5: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9485999941825867\n",
      "Epoch 6: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9557999968528748\n",
      "Epoch 7: Train Accuracy - 0.9200000166893005, Val Accuracy - 0.9592000246047974\n",
      "Epoch 8: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9624000191688538\n",
      "Epoch 9: Train Accuracy - 0.8999999761581421, Val Accuracy - 0.9629999995231628\n",
      "Epoch 10: Train Accuracy - 0.9200000166893005, Val Accuracy - 0.9643999934196472\n",
      "Epoch 11: Train Accuracy - 0.9399999976158142, Val Accuracy - 0.967199981212616\n",
      "Epoch 12: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9685999751091003\n",
      "Epoch 13: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9703999757766724\n",
      "Epoch 14: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9733999967575073\n",
      "Epoch 15: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9715999960899353\n",
      "Epoch 16: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9733999967575073\n",
      "Epoch 17: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9739999771118164\n",
      "Epoch 18: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9760000109672546\n",
      "Epoch 19: Train Accuracy - 0.9399999976158142, Val Accuracy - 0.975600004196167\n",
      "Epoch 20: Train Accuracy - 1.0, Val Accuracy - 0.975600004196167\n",
      "Epoch 21: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9765999913215637\n",
      "Epoch 22: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9757999777793884\n",
      "Epoch 23: Train Accuracy - 1.0, Val Accuracy - 0.9765999913215637\n",
      "Epoch 24: Train Accuracy - 1.0, Val Accuracy - 0.9775999784469604\n",
      "Epoch 25: Train Accuracy - 1.0, Val Accuracy - 0.9782000184059143\n",
      "Epoch 26: Train Accuracy - 1.0, Val Accuracy - 0.9779999852180481\n",
      "Epoch 27: Train Accuracy - 1.0, Val Accuracy - 0.977400004863739\n",
      "Epoch 28: Train Accuracy - 1.0, Val Accuracy - 0.9783999919891357\n",
      "Epoch 29: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9793999791145325\n",
      "Epoch 30: Train Accuracy - 1.0, Val Accuracy - 0.979200005531311\n",
      "Epoch 31: Train Accuracy - 1.0, Val Accuracy - 0.9789999723434448\n",
      "Epoch 32: Train Accuracy - 1.0, Val Accuracy - 0.9793999791145325\n",
      "Epoch 33: Train Accuracy - 1.0, Val Accuracy - 0.9797999858856201\n",
      "Epoch 34: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9797999858856201\n",
      "Epoch 35: Train Accuracy - 1.0, Val Accuracy - 0.980400025844574\n",
      "Epoch 36: Train Accuracy - 1.0, Val Accuracy - 0.9797999858856201\n",
      "Epoch 37: Train Accuracy - 1.0, Val Accuracy - 0.9796000123023987\n",
      "Epoch 38: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9793999791145325\n",
      "Epoch 39: Train Accuracy - 1.0, Val Accuracy - 0.980400025844574\n",
      "Epoch 40: Train Accuracy - 1.0, Val Accuracy - 0.9796000123023987\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"Epoch {0}: Train Accuracy - {1}, Val Accuracy - {2}\".format(\n",
    "            epoch + 1, acc_train, acc_val))\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Run the below codes to instead save log information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9097999930381775\n",
      "Epoch 2: Train Accuracy - 0.8999999761581421, Val Accuracy - 0.9279999732971191\n",
      "Epoch 3: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9373999834060669\n",
      "Epoch 4: Train Accuracy - 0.9399999976158142, Val Accuracy - 0.9437999725341797\n",
      "Epoch 5: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9480000138282776\n",
      "Epoch 6: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9539999961853027\n",
      "Epoch 7: Train Accuracy - 0.9399999976158142, Val Accuracy - 0.9589999914169312\n",
      "Epoch 8: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9624000191688538\n",
      "Epoch 9: Train Accuracy - 1.0, Val Accuracy - 0.9624000191688538\n",
      "Epoch 10: Train Accuracy - 1.0, Val Accuracy - 0.9656000137329102\n",
      "Epoch 11: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.967199981212616\n",
      "Epoch 12: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9684000015258789\n",
      "Epoch 13: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9700000286102295\n",
      "Epoch 14: Train Accuracy - 1.0, Val Accuracy - 0.9715999960899353\n",
      "Epoch 15: Train Accuracy - 1.0, Val Accuracy - 0.973800003528595\n",
      "Epoch 16: Train Accuracy - 1.0, Val Accuracy - 0.9711999893188477\n",
      "Epoch 17: Train Accuracy - 1.0, Val Accuracy - 0.9742000102996826\n",
      "Epoch 18: Train Accuracy - 1.0, Val Accuracy - 0.9771999716758728\n",
      "Epoch 19: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.9751999974250793\n",
      "Epoch 20: Train Accuracy - 1.0, Val Accuracy - 0.9764000177383423\n",
      "Epoch 21: Train Accuracy - 1.0, Val Accuracy - 0.9769999980926514\n",
      "Epoch 22: Train Accuracy - 1.0, Val Accuracy - 0.9747999906539917\n",
      "Epoch 23: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9775999784469604\n",
      "Epoch 24: Train Accuracy - 1.0, Val Accuracy - 0.9765999913215637\n",
      "Epoch 25: Train Accuracy - 0.9599999785423279, Val Accuracy - 0.977400004863739\n",
      "Epoch 26: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9768000245094299\n",
      "Epoch 27: Train Accuracy - 1.0, Val Accuracy - 0.9797999858856201\n",
      "Epoch 28: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9778000116348267\n",
      "Epoch 29: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9787999987602234\n",
      "Epoch 30: Train Accuracy - 1.0, Val Accuracy - 0.9783999919891357\n",
      "Epoch 31: Train Accuracy - 1.0, Val Accuracy - 0.9789999723434448\n",
      "Epoch 32: Train Accuracy - 1.0, Val Accuracy - 0.9787999987602234\n",
      "Epoch 33: Train Accuracy - 1.0, Val Accuracy - 0.9807999730110168\n",
      "Epoch 34: Train Accuracy - 1.0, Val Accuracy - 0.979200005531311\n",
      "Epoch 35: Train Accuracy - 1.0, Val Accuracy - 0.9805999994277954\n",
      "Epoch 36: Train Accuracy - 1.0, Val Accuracy - 0.9797999858856201\n",
      "Epoch 37: Train Accuracy - 0.9800000190734863, Val Accuracy - 0.9807999730110168\n",
      "Epoch 38: Train Accuracy - 1.0, Val Accuracy - 0.9793999791145325\n",
      "Epoch 39: Train Accuracy - 1.0, Val Accuracy - 0.9807999730110168\n",
      "Epoch 40: Train Accuracy - 1.0, Val Accuracy - 0.9801999926567078\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration % 10 == 0:\n",
    "                summary_str = acc_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                # step is total number of minibatches from beginning\n",
    "                step = epoch * (mnist.train.num_examples // batch_size + 1) + iteration\n",
    "                # call the file_writer to add the above information\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(\"Epoch {0}: Train Accuracy - {1}, Val Accuracy - {2}\".format(\n",
    "            epoch + 1, acc_train, acc_val))\n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Stored Model on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore( sess, \"./ my_model_final.ckpt\")\n",
    "    X_new_scaled = [...] # some new images (scaled from 0 to 1)\n",
    "    Z = logits.eval(feed_dict ={ X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf35",
   "language": "python",
   "name": "tf35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
